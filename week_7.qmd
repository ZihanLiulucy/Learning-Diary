---
title: "week_7"
author: "Zihan Liu"
---

## Summary

### Introduction

This week's key point is classifying remotely sensed data, a process of categorizing areas in images. The primary method is machine learning, a subset of computer science developing algorithms that allow computers to learn and make decisions independently. This learning process is similar with human inference, where experiences are generalized to form conclusions, enabling machines to do classification and analysis effectively, without requirement of explicit programming for each specific task.

### Classification methods

#### Classification Trees

![](classification%20tree.png)

Purpose: Classify data into discrete categories based on certain features.

Example: Deciding whether to play golf based on weather conditions (temperature, rainfall, wind).

Classification trees, similar with flowcharts, systematically classify data into discrete categories based on its features, like deciding to play golf depending on weather conditions. Starting from the root node, a split is established by solving an optimization problem (usually minimizing an impurity measure), before proceeding to recurse on the two resulting child nodes [@BertsimasDunn2017]. Each node in the tree makes a decision, categorizing the data into different paths based on specific criteria or thresholds. The decision to split at each node is often based on criteria such as Gini impurity, the goal is to choose splits that decrease Gini impurity, leading to a more accurate classification.

Output variable: categorical

#### Regression Trees

Purpose: Predict a continuous dependent variable.

Example: Predicting GCSE scores

Data is divided into smaller subsets using decision trees, allowing for more precise predictions in cases where a simple linear model fails. The decision to split data is based on reducing the sum of squared residuals (SSR), aiming for the lowest SSR at each split. The initial split (root of the tree) is chosen based on the threshold that minimizes SSR, and this process is repeated for subsequent splits. To avoid overfitting, a minimum number of observations can be required before further splitting.

Output variable: continuous

#### overfitting:

![](overfitting.png)

Overfitting, with high variance and low bias occurs when a model learns the training data too well. This happens often with very complex models that have too many parameters relative to the number of observations. While such a model may perform well on the training data, but drops significantly on new, unseen data.

Underfitting, with high bias and low variance, occurs when a model is too simple to capture the underlying structure of the data. This can happen if the model does not have enough parameters (or complexity) to learn from the data.

We are aiming for good balance with low bias and low variance, performs well on the training data and maintains good performance on new, unseen data.

#### Random Forests

![](RF.png) Figure from [@BelgiuDragut2016]

A random forest (RF) classifier is an ensemble classifier that uses a randomly chosen subset of training samples and variables to generate several decision trees [@BelgiuDragut2016]. In contrast to alternative approaches, the RFR model can handle large data dimensionality and multicollinearity and is less sensitive to noise and overfitting [@WangEtAl2016]

It enhance decision tree performance by aggregating predictions from multiple trees, reducing overfitting and improving accuracy, uses random samples with replacement to create diverse trees. At each split, selects a random subset of features, increasing tree diversity. Repeats sampling and feature selection to create many trees, forming a forest. Trees grow to their full size without pruning, relying on the ensemble to prevent overfitting.

#### Unsupervised classification

Unsupervised classification, often called clustering, includes techniques like k-means and DBSCAN, which categorize data based on features like spectral space and distance metrics.

#### Supervised classification

Supervised classification is a method used to categorize data into predefined groups or classes based on training data that is already labeled.

#### Support Vector Machine (SVM)

A powerful tool in machine learning for sorting data into categories. Draw a line (or a plane in higher dimensions when there are more then 2 datasets) that best separates different types of data points.

SVM looks for the line that keeps the maximum distance from the closest points of any category, ensuring it's not just separating but also maximizing the space between these categories. These closest points are called support vectors.

Sometimes data isn't easily separable with a straight line. SVM can twist and turn the data (using something called the kernel trick) to find a way to separate it effectively.

### Classification workshop

The primary emphasis here is on classification, involving the process of training a model with some samples and then using this model to categorize the rest of the image.

![](week_7_workshop.png)

light pink: urban low; violet: water; dark pink: urban high; light yellow: grass; gray: bare earth, dark green: forest

The city I pick here is York, and the classification follow urban high and low, water, grass, forest, and bare earth. I picked some samples of them on the map, however I find it hard to determine urban high and low, as well as grass and forest, as they look very similar on the satellite maps. Thus I need to further check the street image on google map to ensure the accuracy. And another issue is that the selected training areas cannot exceed the limit, so it is important to define the most accurate samples.

This map shows an reasonable classification of urban features, the distinguish between urban, greenery, and water is clear, but it is not sure whether the urban high and low, as well as grass and forest has been well classified.

## Application

This week I want to look deeply into the classification methods to consolidate the knowledge as well as help with selecting proper classification tool in further study.

### Application 1

Classification and regression trees are ideally suited for the analysis of complex ecological data. In this study [@DeathFabricius2000] we evaluate survey data, including physical and spatial environmental variables and abundances of soft coral species from the Australian central Great Barrier Reef using regression trees and categorization. Dense aggregations, usually consisting of three taxa, were found to be limited to specific habitat categories, each of which was determined by a combination of three to four environmental variables, according to regression tree analyses. The study found that both physical and spatial variables were effective predictors of soft coral abundances, and spatial variables could replace physical variables in extensive reef complexes where physical data might be unavailable. The case study also illustrated the advantage of CART over linear models in uncovering patterns in the dataâ€‹

![](CART.png)

Fig from [@DeathFabricius2000]

The regression tree shows that visibility was the best-explained physical variable, sediment is the next, and finally, slope and waves. For all the variables, the cross-shelf position effect the most, followed by depth location and reef type shows relatively small impact.

### Application 2

In the case study conducted in Bangladesh [@ZhaoEtAl2019], the Random Forest Regression (RFR) model was used to estimate poverty using data from multiple sources, including nighttime light data, Google satellite imagery, land cover map, road map, and division headquarter location data. The household wealth index (WI) from the Demographic and Health Surveys (DHS) was the measure of poverty. The RFR model's effectiveness stems from its ability to handle various data types, manage high dimensionality, and cope with multicollinearity, leading to a more accurate and reliable poverty estimation compared to traditional methods.

Here is the dataset used in this study.

![](RFR.png){width="654"}

Fig from [@ZhaoEtAl2019]

(a)Wealth Index (WI) map, (b) National Polar-orbiting Partnership Visible Infrared Imaging Radiometer Suite (NPP-VIIRS) nighttime light (NTL) image, (c) Open Street Map (OSM) primary and secondary road map, (d) land cover map

The model demonstrated good predictive power and generalization ability. The use of The methodology was efficient in measuring poverty due to RFR's robust handling of complex and varied data

### Application reflection

The two articles show the application of machine learning techniques. RFR is effective in poverty measurement, as poverty is multifaceted, influenced by various socioeconomic and environmental factors. Also RFR can handle multicollinearity better then CART, as in social sciences, many variables can be interrelated, which complicates the analysis.

## Reflection

This week's exploration of classifying remotely sensed data using machine learning is similar with human inference. What I found useful in the further study is classification trees and regression trees, with applications ranging from weather-based activities to predicting educational outcomes. The concept of overfitting and underfitting brought me back to the memory of learning regression models.

Random Forests (RF) gave me an idea of how to avoid the bad effects of multicollinearity similar with the knowledge from multi-linear regression. It can handle large datasets and complex interactions through gathering decision trees, reducing overfitting and enhancing predictive accuracy. This was practically useful in studies like coral species analysis and poverty estimation.

Unsupervised methods like k-means and supervised techniques, including Support Vector Machines (SVM), expanded my understanding of machine learning's diversity.Through these insights, I gained a comprehensive view of machine learning's role in data analysis, from theoretical foundations to real-world applications.

## **References**
